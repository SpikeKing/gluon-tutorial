> MXNet: A flexible and efficient library for deep learning.

è¿™æ˜¯MXNetçš„[å®˜ç½‘](https://mxnet.incubator.apache.org/)ä»‹ç»ï¼Œâ€œMXNetæ˜¯çµæ´»ä¸”é«˜æ•ˆçš„æ·±åº¦å­¦ä¹ åº“â€ã€‚

![MXNet](./doc/logo.png)

MXNetæ˜¯ä¸»æµçš„ä¸‰å¤§æ·±åº¦å­¦ä¹ æ¡†æ¶ä¹‹ä¸€ï¼š

- [TensorFlow](https://www.tensorflow.org/)ï¼šGoogleæ”¯æŒï¼Œå…¶ç®€åŒ–ç‰ˆæ˜¯[Keras](https://keras.io/)ï¼›
- [PyTorch](https://pytorch.org/)ï¼šFacebookæ”¯æŒï¼Œå…¶å·¥ä¸šç‰ˆæ˜¯[Caffe2](https://caffe2.ai/)ï¼›
- MXNetï¼šä¸­ç«‹ï¼ŒApacheå­µåŒ–å™¨é¡¹ç›®ï¼Œä¹Ÿè¢«AWSé€‰ä¸ºå®˜æ–¹DLå¹³å°ï¼›

MXNetçš„ä¼˜åŠ¿æ˜¯ï¼Œå…¶å¼€å‘è€…ä¹‹ä¸€ææ²ï¼Œæ˜¯ä¸­å›½äººğŸ‡¨ğŸ‡³ï¼Œåœ¨MXNetçš„æ¨å¹¿ä¸­å…·æœ‰è¯­è¨€ä¼˜åŠ¿ï¼ˆæ±‰è¯­ï¼‰ï¼Œæœ‰åˆ©äºå›½å†…å¼€å‘è€…çš„å­¦ä¹ ã€‚åŒæ—¶ï¼Œæ¨èææ²å½•åˆ¶çš„[æ•™å­¦è§†é¢‘](https://discuss.gluon.ai/t/topic/753)ï¼Œéå¸¸ä¸é”™ã€‚

MXNetçš„é«˜å±‚æ¥å£æ˜¯Gluonï¼ŒGluonåŒæ—¶æ”¯æŒçµæ´»çš„åŠ¨æ€å›¾å’Œé«˜æ•ˆçš„é™æ€å›¾ï¼Œæ—¢ä¿ç•™åŠ¨æ€å›¾çš„æ˜“ç”¨æ€§ï¼Œä¹Ÿå…·æœ‰é™æ€å›¾çš„é«˜æ€§èƒ½ï¼Œè¿™ä¹Ÿæ˜¯å®˜ç½‘ä»‹ç»çš„*flexible*å’Œ*efficient*çš„å‡ºå¤„ã€‚åŒæ—¶ï¼ŒMXNetè¿˜å…·å¤‡å¤§é‡å­¦æœ¯ç•Œçš„å‰æ²¿ç®—æ³•ï¼Œæ–¹ä¾¿ç§»æ¤è‡³å·¥ä¸šç•Œã€‚å¸Œæœ›MXNetå›¢é˜Ÿå†æ¥å†åŠ±ï¼Œåœ¨æ·±åº¦å­¦ä¹ æ¡†æ¶çš„ç«èµ›ä¸­ï¼Œä½äºå‰åˆ—ã€‚

å› æ­¤ï¼ŒæŒæ¡ **MXNet/Gluon** å¾ˆæœ‰å¿…è¦ã€‚

æœ¬æ–‡ä»¥æ·±åº¦å­¦ä¹ çš„**å¤šå±‚æ„ŸçŸ¥æœº**ï¼ˆMultilayer Perceptronsï¼‰ä¸ºç®—æ³•åŸºç¡€ï¼Œæ•°æ®é›†é€‰ç”¨[MNIST](http://yann.lecun.com/exdb/mnist/)ï¼Œä»‹ç»MXNetçš„å·¥ç¨‹ç»†èŠ‚ã€‚

æœ¬æ–‡çš„[æºç ](https://github.com/SpikeKing/gluon-tutorial)ï¼šhttps://github.com/SpikeKing/gluon-tutorial

---

## æ•°æ®é›†

åœ¨è™šæ‹Ÿç¯å¢ƒï¼ˆVirtual Envï¼‰ä¸­ï¼Œç›´æ¥ä½¿ç”¨pipå®‰è£…MXNetå³å¯ï¼š

``` text
pip install mxnet
```

å¦‚æœä¸‹è½½é€Ÿåº¦è¾ƒæ…¢ï¼Œæ¨èä½¿ç”¨**é˜¿é‡Œäº‘**çš„pypiæºï¼š

``` text
-i http://mirrors.aliyun.com/pypi/simple --trusted-host mirrors.aliyun.com
```

[MNIST](http://yann.lecun.com/exdb/mnist/)å°±æ˜¯è‘—åçš„æ‰‹å†™æ•°å­—è¯†åˆ«åº“ï¼Œå…¶ä¸­åŒ…å«0è‡³9ç­‰10ä¸ªæ•°å­—çš„æ‰‹å†™ä½“ï¼Œå›¾ç‰‡å¤§å°ä¸º28*28çš„ç°åº¦å›¾ï¼Œç›®æ ‡æ˜¯æ ¹æ®å›¾ç‰‡è¯†åˆ«æ­£ç¡®çš„æ•°å­—ã€‚

MNISTåº“åœ¨MXNetä¸­è¢«å°è£…ä¸º**MNISTç±»**ï¼Œæ•°æ®å­˜å‚¨äº``.mxnet/datasets/mnist``ä¸­ã€‚å¦‚æœä¸‹è½½MNISTæ•°æ®è¾ƒæ…¢ï¼Œå¯ä»¥é€‰æ‹©åˆ°[MNIST](http://yann.lecun.com/exdb/mnist/)å®˜ç½‘ä¸‹è½½ï¼Œæ”¾å…¥mnistæ–‡ä»¶å¤¹ä¸­å³å¯ã€‚åœ¨MNISTç±»ä¸­ï¼š

- å‚æ•°``train``ï¼šæ˜¯å¦ä¸ºè®­ç»ƒæ•°æ®ï¼Œå…¶ä¸­trueæ˜¯è®­ç»ƒæ•°æ®ï¼Œfalseæ˜¯æµ‹è¯•æ•°æ®ï¼›
- å‚æ•°``transform``ï¼šæ•°æ®çš„è½¬æ¢å‡½æ•°ï¼Œlambdaè¡¨è¾¾å¼ï¼Œè½¬æ¢æ•°æ®å’Œæ ‡ç­¾ä¸ºæŒ‡å®šçš„æ•°æ®ç±»å‹ï¼›

æºç ï¼š

``` python
# å‚æ•°train
if self._train:
    data, label = self._train_data, self._train_label
else:
    data, label = self._test_data, self._test_label
    
# å‚æ•°transform
if self._transform is not None:
    return self._transform(self._data[idx], self._label[idx])
return self._data[idx], self._label[idx]
```

åœ¨MXNetä¸­ï¼Œæ•°æ®åŠ è½½ç±»è¢«å°è£…æˆ**DataLoaderç±»**ï¼Œè¿­ä»£å™¨æ¨¡å¼ï¼Œè¿­ä»£è¾“å‡ºä¸æ‰¹æ¬¡æ•°ç›¸åŒçš„æ ·æœ¬é›†ã€‚åœ¨DataLoaderä¸­ï¼Œ

- å‚æ•°``dataset``ï¼šæ•°æ®æºï¼Œå¦‚MNISTï¼›
- å‚æ•°``batch_size``ï¼šè®­ç»ƒä¸­çš„æ‰¹æ¬¡æ•°é‡ï¼Œåœ¨è¿­ä»£ä¸­è¾“å‡ºæŒ‡å®šæ•°é‡çš„æ ·æœ¬ï¼›
- å‚æ•°``shuffle``ï¼šæ˜¯å¦æ´—ç‰Œï¼Œå³æ‰“ä¹±æ•°æ®ï¼Œä¸€èˆ¬åœ¨è®­ç»ƒæ—¶éœ€è¦æ­¤æ“ä½œã€‚

è¿­ä»£å™¨çš„æµ‹è¯•ï¼Œæ¯æ¬¡è¾“å‡ºæ ·æœ¬ä¸ªæ•°ï¼ˆç¬¬1ç»´ï¼‰ä¸æŒ‡å®šçš„æ‰¹æ¬¡æ•°é‡ç›¸åŒï¼š

``` python
for data, label in train_data:
    print(data.shape)  # (64L, 28L, 28L, 1L)
    print(label.shape)  # (64L,)
    break
```

åœ¨``load_data()``æ–¹æ³•ä¸­ï¼Œè¾“å‡ºè®­ç»ƒå’Œæµ‹è¯•æ•°æ®ï¼Œæ•°æ®ç±»å‹æ˜¯0~1ï¼ˆç°åº¦å€¼é™¤ä»¥255ï¼‰çš„æµ®ç‚¹æ•°ï¼Œæ ‡ç­¾ç±»å‹ä¹Ÿæ˜¯æµ®ç‚¹æ•°ã€‚

å…·ä½“å®ç°ï¼š

``` python
def load_data(self):
    def transform(data, label):
        return data.astype(np.float32) / 255., label.astype(np.float32)
    train_data = DataLoader(MNIST(train=True, transform=transform),
                            self.batch_size, shuffle=True)
    test_data = DataLoader(MNIST(train=False, transform=transform),
                           self.batch_size, shuffle=False)
    return train_data, test_data
```

---

## æ¨¡å‹

ç½‘ç»œæ¨¡å‹ä½¿ç”¨MXNetä¸­Gluonçš„æ ·å¼ï¼š

1. åˆ›å»º``Sequential()``åºåˆ—ï¼ŒSequentialæ˜¯å…¨éƒ¨æ“ä½œå•å…ƒçš„å®¹å™¨ï¼›
2. æ·»åŠ å…¨è¿æ¥å•å…ƒDenseï¼Œå‚æ•°unitsæ˜¯è¾“å‡ºå•å…ƒçš„ä¸ªæ•°ï¼Œå‚æ•°activationæ˜¯æ¿€æ´»å‡½æ•°ï¼›
3. åˆå§‹åŒ–å‚æ•°ï¼š
    - initæ˜¯æ•°æ®æ¥æºï¼ŒNormalç±»å³æ­£æ€åˆ†å¸ƒï¼Œsigmaæ˜¯æ­£æ€åˆ†å¸ƒçš„æ ‡å‡†å·®ï¼›
    - ctxæ˜¯ä¸Šä¸‹æ–‡ï¼Œè¡¨ç¤ºè®­ç»ƒä¸­å‚æ•°æ›´æ–°ä½¿ç”¨CPUæˆ–GPUï¼Œå¦‚mx.cpu()ï¼›

Gluonçš„Sequentialç±»ä¸å…¶ä»–çš„æ·±åº¦å­¦ä¹ æ¡†æ¶ç±»ä¼¼ï¼Œé€šè¿‡**æœ‰åºåœ°**è¿æ¥ä¸åŒçš„æ“ä½œå•å…ƒï¼Œç»„æˆä¸åŒçš„ç½‘ç»œç»“æ„ï¼Œæ¯ä¸€å±‚åªéœ€è®¾ç½®è¾“å‡ºçš„ç»´åº¦ï¼Œè¾“å…¥ç»´åº¦é€šè¿‡ä¸Šä¸€å±‚ä¼ é€’ï¼Œè½¬æ¢çŸ©é˜µåœ¨å†…éƒ¨è‡ªåŠ¨è®¡ç®—ã€‚

å®ç°ï¼š

``` python
def model(self):
    num_hidden = 64
    net = gluon.nn.Sequential()
    with net.name_scope():
        net.add(gluon.nn.Dense(units=num_hidden, activation="relu"))
        net.add(gluon.nn.Dense(units=num_hidden, activation="relu"))
        net.add(gluon.nn.Dense(units=self.num_outputs))

    net.collect_params().initialize(init=mx.init.Normal(sigma=.1), ctx=self.model_ctx)
    print(net)  # å±•ç¤ºæ¨¡å‹
    return net
```

å…¶ä¸­ï¼Œ``net.name_scope()``ä¸ºSequentialä¸­çš„æ“ä½œå•å…ƒè‡ªåŠ¨æ·»åŠ åç§°ã€‚

### [æ¨¡å‹å¯è§†åŒ–](https://mxnet.incubator.apache.org/faq/visualize_graph.html)

ç›´æ¥ä½¿ç”¨print()ï¼Œæ‰“å°æ¨¡å‹ç»“æ„ï¼Œå¦‚``print(net)``ï¼š

``` text
Sequential(
  (0): Dense(None -> 64, Activation(relu))
  (1): Dense(None -> 64, Activation(relu))
  (2): Dense(None -> 10, linear)
)
```

æˆ–ï¼Œä½¿ç”¨ç¨å¤æ‚çš„**jupyter**ç»˜åˆ¶æ¨¡å‹ï¼Œå®‰è£…jupyteråŒ…ï¼ˆPython 2.xï¼‰ï¼š

``` text
pip install ipython==5.3.0
pip install jupyter==1.0.0
```

å¯åŠ¨jupyteræœåŠ¡ï¼Œè®¿é—®``http://localhost:8888/``ï¼š

``` text
jupyter notebook
```

æ–°å»º``Python 2``æ–‡ä»¶ï¼Œç¼–å†™ç»˜åˆ¶ç½‘ç»œçš„ä»£ç ã€‚ä»£ç çš„æ ·å¼æ˜¯ï¼Œåœ¨å·²æœ‰æ¨¡å‹ä¹‹åï¼Œæ·»åŠ â€œç»˜åˆ¶é€»è¾‘â€ï¼Œè°ƒç”¨``plot_network()``å³å¯ç»˜å›¾ã€‚å¦‚æœæ›¿æ¢Sequentialç±»ä¸ºHybridSequentialç±»ï¼Œå¯ä»¥æå‡ç»˜åˆ¶æ•ˆç‡ï¼Œä¸æ›¿æ¢ä¹Ÿä¸ä¼šå½±å“ç»˜åˆ¶æ•ˆæœ

ç½‘ç»œæ¨¡å‹å’Œç»˜åˆ¶é€»è¾‘ï¼š

``` python
import mxnet as mx
from mxnet import gluon

num_hidden = 64
net = gluon.nn.HybridSequential()
with net.name_scope():
    net.add(gluon.nn.Dense(num_hidden, activation="relu"))
    net.add(gluon.nn.Dense(num_hidden, activation="relu"))
    net.add(gluon.nn.Dense(10))

# ç»˜åˆ¶é€»è¾‘
net.hybridize()
net.collect_params().initialize()
x = mx.sym.var('data')
sym = net(x)
mx.viz.plot_network(sym)
```

æ•ˆæœå›¾ï¼š

![æ¨¡å‹]((./doc/net.png))

---

## è®­ç»ƒ

åœ¨è®­ç»ƒå‰ï¼ŒåŠ è½½æ•°æ®ï¼Œåˆ›å»ºç½‘ç»œã€‚

``` python
train_data, test_data = self.load_data()  # è®­ç»ƒå’Œæµ‹è¯•æ•°æ®
net = self.model()  # æ¨¡å‹
```

æ¥ç€ï¼Œåˆ›å»ºäº¤å‰ç†µçš„æ¥å£``softmax_cross_entropy``ï¼Œåˆ›å»ºè®­ç»ƒå™¨``trainer``ã€‚

è®­ç»ƒå™¨çš„å‚æ•°åŒ…å«ï¼šç½‘ç»œä¸­å‚æ•°ã€ä¼˜åŒ–å™¨ã€ä¼˜åŒ–å™¨çš„å‚æ•°ç­‰ã€‚

``` python
epochs = 10
smoothing_constant = .01
num_examples = 60000

softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()  # äº¤å‰ç†µ
trainer = gluon.Trainer(params=net.collect_params(),
                        optimizer='sgd',
                        optimizer_params={'learning_rate': smoothing_constant})
```

å¾ªç¯epochè®­ç»ƒç½‘ç»œæ¨¡å‹ï¼š

1. ä»è¿­ä»£å™¨``train_data``æºä¸­ï¼Œè·å–æ‰¹æ¬¡æ•°æ®å’Œæ ‡ç­¾ï¼š
2. æŒ‡å®šæ•°æ®å’Œæ ‡ç­¾çš„æ‰§è¡Œç¯å¢ƒctxæ˜¯CPUæˆ–GPUï¼ŒåŒæ—¶å±•å¼€æ•°æ®ä¸º1è¡Œï¼›
3. è‡ªåŠ¨æ¢¯åº¦è®¡ç®—``autograd.record()``ï¼Œç½‘ç»œé¢„æµ‹æ•°æ®ï¼Œè¾“å‡ºoutputï¼Œè®¡ç®—äº¤å‰ç†µlossï¼›
4. å¯¹äºlossåå‘ä¼ æ’­æ±‚å¯¼ï¼Œè®¾ç½®è®­ç»ƒå™¨trainerçš„æ­¥éª¤ä¸ºæ‰¹æ¬¡æ•°ï¼›
4. åœ¨``cumulative_loss``ä¸­ï¼Œç´¯åŠ æ¯ä¸ªæ‰¹æ¬¡çš„æŸå¤±lossï¼Œè®¡ç®—å…¨éƒ¨æŸå¤±ï¼›
5. åœ¨è®­ç»ƒä¸€æ¬¡epochä¹‹åï¼Œè®¡ç®—æµ‹è¯•å’Œè®­ç»ƒæ•°æ®çš„å‡†ç¡®ç‡accuracyï¼›

ä¸æ–­å¾ªç¯ï¼Œç›´è‡³æ‰§è¡Œå®Œæˆå…¨éƒ¨epochsä¸ºæ­¢ã€‚

è®­ç»ƒçš„å®ç°ï¼š

``` python
for e in range(epochs):
    cumulative_loss = 0  # ç´¯ç§¯çš„
    for i, (data, label) in enumerate(train_data):
        data = data.as_in_context(self.model_ctx).reshape((-1, 784))  # æ•°æ®
        label = label.as_in_context(self.model_ctx)  # æ ‡ç­¾

        with autograd.record():  # æ¢¯åº¦
            output = net(data)  # è¾“å‡º
            loss = softmax_cross_entropy(output, label)  # è¾“å…¥å’Œè¾“å‡ºè®¡ç®—loss

        loss.backward()  # åå‘ä¼ æ’­
        trainer.step(data.shape[0])  # è®¾ç½®trainerçš„step
        cumulative_loss += nd.sum(loss).asscalar()  # è®¡ç®—å…¨éƒ¨æŸå¤±

    test_accuracy = self.__evaluate_accuracy(test_data, net)
    train_accuracy = self.__evaluate_accuracy(train_data, net)
    print("Epoch %s. Loss: %s, Train_acc %s, Test_acc %s" %
          (e, cumulative_loss / num_examples, train_accuracy, test_accuracy))
```

åœ¨é¢„æµ‹æ¥å£``evaluate_accuracy()``ä¸­ï¼š

1. åˆ›å»ºå‡†ç¡®ç‡Accuracyç±»accï¼Œç”¨äºç»Ÿè®¡å‡†ç¡®ç‡ï¼›
2. è¿­ä»£è¾“å‡ºæ‰¹æ¬¡çš„æ•°æ®å’Œæ ‡ç­¾ï¼›
3. é¢„æµ‹æ•°æ®ä¸åŒç±»åˆ«çš„æ¦‚ç‡ï¼Œé€‰æ‹©æœ€å¤§æ¦‚ç‡ï¼ˆargmaxï¼‰åšä¸ºç±»åˆ«ï¼›
4. é€šè¿‡``acc.update()``æ›´æ–°å‡†ç¡®ç‡ï¼›

æœ€ç»ˆè¿”å›å‡†ç¡®ç‡çš„å€¼ï¼Œå³accçš„ç¬¬2ç»´``acc[1]``ï¼Œè€Œaccçš„ç¬¬1ç»´``acc[0]``æ˜¯accçš„åç§°ã€‚

``` python
def __evaluate_accuracy(self, data_itertor, net):
    acc = mx.metric.Accuracy()  # å‡†ç¡®ç‡
    for i, (data, label) in enumerate(data_iterator):
        data = data.as_in_context(self.model_ctx).reshape((-1, 784))
        label = label.as_in_context(self.model_ctx)
        output = net(data)  # é¢„æµ‹ç»“æœ
        predictions = nd.argmax(output, axis=1)  # ç±»åˆ«
        acc.update(preds=predictions, labels=label)  # æ›´æ–°æ¦‚ç‡å’Œæ ‡ç­¾
    return acc.get()[1]  # ç¬¬1ç»´æ˜¯æ•°æ®åç§°ï¼Œç¬¬2ç»´æ˜¯æ¦‚ç‡
```

æ•ˆæœï¼š

``` python
Epoch 0. Loss: 1.2743850797812144, Train_acc 0.846283333333, Test_acc 0.8509
Epoch 1. Loss: 0.46071574948628746, Train_acc 0.884366666667, Test_acc 0.8892
Epoch 2. Loss: 0.37149955205917357, Train_acc 0.896466666667, Test_acc 0.9008
Epoch 3. Loss: 0.3313815038919449, Train_acc 0.908366666667, Test_acc 0.9099
Epoch 4. Loss: 0.30456133014361064, Train_acc 0.915966666667, Test_acc 0.9172
Epoch 5. Loss: 0.2827877395868301, Train_acc 0.919466666667, Test_acc 0.9214
Epoch 6. Loss: 0.2653073514064153, Train_acc 0.925433333333, Test_acc 0.9289
Epoch 7. Loss: 0.25018166739145914, Train_acc 0.92965, Test_acc 0.9313
Epoch 8. Loss: 0.23669789231618246, Train_acc 0.933816666667, Test_acc 0.9358
Epoch 9. Loss: 0.22473177655935286, Train_acc 0.934716666667, Test_acc 0.9337
```

## GPU

å¯¹äºæ·±åº¦å­¦ä¹ è€Œè¨€ï¼Œä½¿ç”¨GPUå¯ä»¥åŠ é€Ÿç½‘ç»œçš„è®­ç»ƒè¿‡ç¨‹ï¼ŒMXNetåŒæ ·æ”¯æŒä½¿ç”¨GPUè®­ç»ƒç½‘ç»œã€‚

æ£€æŸ¥æœåŠ¡å™¨çš„Cudaç‰ˆæœ¬ï¼Œå‘½ä»¤ï¼š``nvcc --version``ï¼Œç”¨äºç¡®å®šä¸‹è½½MXNetçš„GPUç‰ˆæœ¬ã€‚

``` text
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2016 NVIDIA Corporation
Built on Sun_Sep__4_22:14:01_CDT_2016
Cuda compilation tools, release 8.0, V8.0.44
```

åˆ™ï¼Œå½“å‰æœåŠ¡å™¨çš„Cudaç‰ˆæœ¬æ˜¯8.0ã€‚

å°†MXNetç”±CPUç‰ˆæœ¬è½¬ä¸º[GPUç‰ˆæœ¬](https://mxnet.incubator.apache.org/install/index.html)ï¼Œå¸è½½``mxnet``ï¼Œå®‰è£…``mxnet-cu80``ã€‚

``` text
pip uninstall mxnet
pip install mxnet-cu80
```

å½“å®‰è£…å®ŒæˆGPUç‰ˆæœ¬ä¹‹åï¼Œåœ¨Python Consoleä¸­ï¼Œæ‰§è¡Œå¦‚ä¸‹ä»£ç ï¼Œç¡®è®¤MXNetçš„GPUåº“å¯ä»¥ä½¿ç”¨ã€‚

``` python
>>> import mxnet as mx
>>> a = mx.nd.ones((2, 3), mx.gpu())
>>> b = a * 2 + 1
>>> b.asnumpy()
array([[ 3.,  3.,  3.],
       [ 3.,  3.,  3.]], dtype=float32)
```

æ£€æŸ¥GPUæ•°é‡ï¼Œå‘½ä»¤ï¼š``nvidia-smi``ï¼š

``` text
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 375.26                 Driver Version: 375.26                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  TITAN X (Pascal)    Off  | 0000:02:00.0     Off |                  N/A |
| 28%   49C    P2    84W / 250W |  12126MiB / 12189MiB |     25%      Default |
+-------------------------------+----------------------+----------------------+
|   1  TITAN X (Pascal)    Off  | 0000:03:00.0     Off |                  N/A |
| 24%   39C    P2    57W / 250W |  12126MiB / 12189MiB |     33%      Default |
+-------------------------------+----------------------+----------------------+
|   2  TITAN X (Pascal)    Off  | 0000:83:00.0     Off |                  N/A |
| 25%   41C    P2    58W / 250W |  12126MiB / 12189MiB |     37%      Default |
+-------------------------------+----------------------+----------------------+
|   3  TITAN X (Pascal)    Off  | 0000:84:00.0     Off |                  N/A |
| 23%   31C    P2    53W / 250W |  11952MiB / 12189MiB |      2%      Default |
+-------------------------------+----------------------+----------------------+
```

åˆ™ï¼Œå½“å‰æœåŠ¡å™¨çš„GPUæ•°é‡æ˜¯4ã€‚

è®¾ç½®å‚æ•°ç¯å¢ƒctxä¸ºGPUçš„åˆ—è¡¨ï¼Œå³``[mx.gpu(0), mx.gpu(1), ...]``ã€‚

``` python
GPU_COUNT = 4
ctx = [mx.gpu(i) for i in range(GPU_COUNT)]
```

åœ¨ç½‘ç»œnetä¸­ä½¿ç”¨GPUåˆå§‹åŒ–initialize()å‚æ•°paramsï¼Œç„¶ååˆ›å»ºtrainerè®­ç»ƒå™¨ã€‚

``` python
net = self.model()  # æ¨¡å‹
net.collect_params().initialize(init=mx.init.Normal(sigma=.1), ctx=ctx)

smoothing_constant = .01
trainer = gluon.Trainer(params=net.collect_params(),
                        optimizer='sgd',
                        optimizer_params={'learning_rate': smoothing_constant})
```

å¾ªç¯æ‰§è¡Œ10ä¸ªepochè®­ç»ƒæ¨¡å‹ï¼Œ``train_data``å’Œ``valid_data``æ˜¯è¿­ä»£å™¨ï¼Œæ¯æ¬¡è¾“å‡ºä¸€ä¸ªbatchæ ·æœ¬é›†ã€‚åœ¨``train_batch()``ä¸­ï¼Œä¾æ¬¡ä¼ å…¥æ‰¹æ¬¡æ•°æ®batchã€GPUç¯å¢ƒåˆ—è¡¨ctxã€ç½‘ç»œnetå’Œè®­ç»ƒå™¨trainerï¼›åœ¨``valid_batch()``ä¸­ï¼Œä¸è®­ç»ƒç±»ä¼¼ï¼Œåªæ˜¯ä¸ä¼ è®­ç»ƒå™¨trainerã€‚

``` python
epochs = 10
for e in range(epochs):
    start = time()
    for batch in train_data:
        self.train_batch(batch, ctx, net, trainer)
    nd.waitall()  # ç­‰å¾…æ‰€æœ‰å¼‚æ­¥çš„ä»»åŠ¡éƒ½ç»ˆæ­¢
    print('Epoch %d, training time = %.1f sec' % (e, time() - start))
    correct, num = 0.0, 0.0
    for batch in valid_data:
        correct += self.valid_batch(batch, ctx, net)
        num += batch[0].shape[0]
    print('\tvalidation accuracy = %.4f' % (correct / num))
```

å…·ä½“åˆ†ææ‰¹æ¬¡è®­ç»ƒæ–¹æ³•``train_batch()``ï¼š

1. è¾“å…¥batchæ˜¯æ•°æ®å’Œæ ‡ç­¾çš„é›†åˆï¼Œç´¢å¼•0è¡¨ç¤ºæ•°æ®ï¼Œç´¢å¼•1è¡¨ç¤ºæ ‡ç­¾ã€‚
2. æ ¹æ®GPUçš„æ•°é‡ï¼Œæ‹†åˆ†æ•°æ®dataä¸æ ‡ç­¾labelï¼Œæ¯ä¸ªGPUå¯¹åº”ä¸åŒçš„æ•°æ®ï¼›
3. æ¯ç»„æ•°æ®å’Œæ ‡ç­¾ï¼Œåˆ†åˆ«åå‘ä¼ æ’­backward()æ›´æ–°ç½‘ç»œnetçš„å‚æ•°ï¼›
4. è®¾ç½®è®­ç»ƒå™¨trainerçš„æ­¥éª¤stepä¸ºæ‰¹æ¬¡æ•°``batch_size``ï¼›

å¤šä¸ªGPUæ˜¯ç›¸äº’ç‹¬ç«‹çš„ï¼Œå› æ­¤ï¼Œå½“ä½¿ç”¨å¤šä¸ªGPUè®­ç»ƒæ¨¡å‹æ—¶ï¼Œéœ€è¦æ³¨æ„ä¸åŒGPUä¹‹é—´çš„æ•°æ®èåˆã€‚

å®ç°å¦‚ä¸‹ï¼š

``` python
@staticmethod
def train_batch(batch, ctx, net, trainer):
    # split the data batch and load them on GPUs
    data = gluon.utils.split_and_load(batch[0], ctx)  # åˆ—è¡¨
    label = gluon.utils.split_and_load(batch[1], ctx)  # åˆ—è¡¨
    # compute gradient
    GluonFirst.forward_backward(net, data, label)
    # update parameters
    trainer.step(batch[0].shape[0])
    
@staticmethod
def forward_backward(net, data, label):
    loss = gluon.loss.SoftmaxCrossEntropyLoss()
    with autograd.record():
        losses = [loss(net(X), Y) for X, Y in zip(data, label)]  # lossåˆ—è¡¨
    for l in losses:  # æ¯ä¸ªlossåå‘ä¼ æ’­
        l.backward()
```

å…·ä½“åˆ†ææ‰¹æ¬¡éªŒè¯æ–¹æ³•``valid_batch()``ï¼š

1. å°†å…¨éƒ¨éªŒè¯æ•°æ®ï¼Œéƒ½è¿è¡Œäºä¸€ä¸ªGPUä¸­ï¼Œå³ctx[0]ï¼›
2. ç½‘ç»œneté¢„æµ‹æ•°æ®dataçš„ç±»åˆ«æ¦‚ç‡ï¼Œå†è½¬æ¢ä¸ºå…·ä½“ç±»åˆ«argmax()ï¼›
3. å°†å…¨éƒ¨é¢„æµ‹æ­£ç¡®çš„æ ·æœ¬è¿›è¡Œæ±‡æ€»ï¼Œè·å¾—æ€»çš„æ­£ç¡®æ ·æœ¬æ•°ï¼›

å®ç°å¦‚ä¸‹ï¼š

``` python
@staticmethod
def valid_batch(batch, ctx, net):
    data = batch[0].as_in_context(ctx[0])
    pred = nd.argmax(net(data), axis=1)
    return nd.sum(pred == batch[1].as_in_context(ctx[0])).asscalar()
```

é™¤äº†è®­ç»ƒéƒ¨åˆ†ï¼ŒGPUçš„æ•°æ®åŠ è½½å’Œç½‘ç»œæ¨¡å‹éƒ½ä¸CPUä¸€è‡´ã€‚

è®­ç»ƒGPUæ¨¡å‹ï¼Œéœ€è¦è¿æ¥è¿œç¨‹æœåŠ¡å™¨ï¼Œä¸Šä¼ å·¥ç¨‹ã€‚å¦‚æœæ— æ³•ä½¿ç”¨Gitä¼ è¾“ï¼Œåˆ™æ¨èä½¿ç”¨[RsyncOSX](https://github.com/rsyncOSX/RsyncOSX)ï¼Œéå¸¸ä¾¿æ·çš„æ–‡ä»¶åŒæ­¥å·¥å…·ï¼š

![RsyncOSX]((./doc/rsync.png))

åœ¨è¿œç¨‹æœåŠ¡å™¨ä¸­ï¼Œå°†å·¥ç¨‹çš„ä¾èµ–åº“å®‰è£…è‡³è™šæ‹Ÿç¯å¢ƒä¸­ï¼Œæ³¨æ„éœ€è¦ä½¿ç”¨MXNetçš„GPUç‰ˆæœ¬``mxnet-cu80``ï¼Œæ¥ç€ï¼Œæ‰§è¡Œæ¨¡å‹è®­ç»ƒã€‚

ä»¥ä¸‹æ˜¯GPUç‰ˆæœ¬çš„æ¨¡å‹è¾“å‡ºç»“æœï¼š

``` text
Epoch 5, training time = 13.7 sec
	validation accuracy = 0.9277
Epoch 6, training time = 13.9 sec
	validation accuracy = 0.9284
Epoch 7, training time = 13.8 sec
	validation accuracy = 0.9335
Epoch 8, training time = 13.7 sec
	validation accuracy = 0.9379
Epoch 9, training time = 14.4 sec
	validation accuracy = 0.9402
```

å½“é‡åˆ°å¦‚ä¸‹è­¦å‘Šâš ï¸æ—¶ï¼š

``` text
only 4 out of 12 GPU pairs are enabled direct access. 
It may affect the performance. You can set MXNET_ENABLE_GPU_P2P=0 to turn it off
```

å…³é—­``MXNET_ENABLE_GPU_P2P``å³å¯ï¼Œä¸å½±å“æ­£å¸¸çš„è®­ç»ƒè¿‡ç¨‹ã€‚

``` text
export MXNET_ENABLE_GPU_P2P=0
```

---

è‡³æ­¤ **MXNet/Gluon** çš„å·¥ç¨‹è®¾è®¡ï¼Œå·²ç»å…¨éƒ¨å®Œæˆï¼Œä»æ•°æ®é›†ã€æ¨¡å‹ã€è®­ç»ƒã€GPUå››ä¸ªéƒ¨åˆ†å‰–æMXNetçš„å®ç°ç»†èŠ‚ï¼ŒMXNetçš„å„ä¸ªç¯èŠ‚è®¾è®¡çš„éå¸¸å·§å¦™ï¼Œä¹Ÿä¸å…¶ä»–æ¡†æ¶ç±»ä¼¼ï¼Œå®¹æ˜“ä¸Šæ‰‹ã€‚å®ä¾‹è™½å°ï¼Œâ€œäº”è„ä¿±å…¨â€ï¼Œä¸ºç»§ç»­å­¦ä¹ MXNetæ¡†æ¶ï¼Œèµ·åˆ°æŠ›ç –å¼•ç‰çš„ä½œç”¨ã€‚

OK, that's all! Enjoy it!




